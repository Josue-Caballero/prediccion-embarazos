{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "proyecto-prediccion_embarazos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Carga de librerias**"
      ],
      "metadata": {
        "id": "eQjDY1VunFl3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNUlLkEFhJdB"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Carga de los datos**\n",
        "Se definió un subconjunto con **1464** registros a partir de los **48700** registros iniciales. Esta división se llevó a cabo dado que se encontró una gran disparidad de datos en la columna que sería la etiqueta de salida (Se encontraron aproximadamente el triple de respuestas **NO** en contraposición a respuestas **SI**). El subconjunto obtenido se mezcló de manera aleatoria para evitar problemas de convergencia al iniciar el entrenamiento. Adicionalmente, del conjunto inicial se extrajo 200 registros para pruebas con el sistema desplegado.\n"
      ],
      "metadata": {
        "id": "OAo-OgnShd3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "##############################################################################\n",
        "data = pd.read_csv('data.csv', encoding='utf-8')\n",
        "\n",
        "subsetYes = data[data['f2_s2_200'] == 'si'].sample(frac=1) # Yes registries\n",
        "subsetYes.reset_index(inplace=True, drop=True)\n",
        "\n",
        "subsetTestYes = subsetYes[1364:] # Registries to test the system\n",
        "subsetYes = subsetYes[:1364]\n",
        "\n",
        "subsetNo = data[data['f2_s2_200'] == 'no'].sample(frac=1) # No registries\n",
        "subsetNo.reset_index(inplace=True, drop=True)\n",
        "\n",
        "subsetTestNo = subsetNo[1364:1464] # Registries to test the system\n",
        "subsetNo = subsetNo[:1364]\n",
        "\n",
        "data = pd.concat([subsetYes, subsetNo], axis=0).sample(frac=1)\n",
        "data.reset_index(inplace=True, drop=True)\n",
        "\n",
        "dataTest = pd.concat([subsetTestYes, subsetTestNo], axis=0)\n",
        "dataTest.reset_index(inplace=True, drop=True)\n",
        "dataTest.to_csv('registriesToTest.csv', index=None)"
      ],
      "metadata": {
        "id": "-Coe-KokhZy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocesamiento**\n",
        "Para empezar, se removieron las columnas que no guardaban relación con la problemática tratada. Se utilizó codificación **OneHot** para los datos categóricos y un **escalamiento estándar** para las edades.\n",
        "\n",
        "**Remplazo de valores**\n",
        "\n",
        "Se remplazo el valor de las celdas vacías o cuyo valor era NaN, pues al revisar el diccionario de datos se concluyó que las celdas en blanco en su mayoría representaban falta de conocimiento  o desinterés por contestar la pregunta.\n",
        "\n",
        "**División del conjunto (entrenamiento y prueba)**\n",
        "\n",
        "Se definió la división de los datos con un 30% para el conjunto de pruebas\n"
      ],
      "metadata": {
        "id": "dUFuPTH6kRah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesing\n",
        "##############################################################################\n",
        "Xenc = OneHotEncoder()\n",
        "scaler = StandardScaler()\n",
        "lencoder = LabelEncoder()\n",
        "omisiones = [\n",
        "    'prov', 'f2_s1_101', 'f2_s6_601_1', 'f2_s6_603_12',\n",
        "    'f2_s8_804', 'f2_s8_806', 'f2_s8_807', 'f2_s8_808', \n",
        "    'f2_s8_809','f2_s8_814', 'f2_s8_821', 'gedad_anios', \n",
        "    'estrato', 'f2_s2_200', 'f2_s2_207'\n",
        "]\n",
        "\n",
        "for x in data.columns:\n",
        "    data[x] = data[x].fillna('no sabe / no responde')\n",
        "    \n",
        "X = data.drop(omisiones, axis=1)\n",
        "\n",
        "y = lencoder.fit_transform(data['f2_s2_200'])\n",
        "\n",
        "Xenc = Xenc.fit(X)\n",
        "\n",
        "XE = Xenc.transform(X).toarray()\n",
        "XE = np.concatenate(\n",
        "    (XE, scaler.fit_transform(pd.DataFrame(data['f2_s1_101']))), \n",
        "    axis=1\n",
        ")\n",
        "\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(\n",
        "    XE, y, test_size=0.30, random_state= 0\n",
        ")\n"
      ],
      "metadata": {
        "id": "cz94gIgRkEtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Entrenamiento**"
      ],
      "metadata": {
        "id": "seYNVGk7mkgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "#############################################################################\n",
        "classifier = LogisticRegression(solver='liblinear')\n",
        "classifier.fit(xTrain, yTrain)\n"
      ],
      "metadata": {
        "id": "13vCgFTVmrzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predicciones**"
      ],
      "metadata": {
        "id": "UB38ahltmuG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "##############################################################################\n",
        "predictions = classifier.predict(xTest)\n",
        "print(classifier.score(xTest, yTest))\n"
      ],
      "metadata": {
        "id": "R7noXH3FmyJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Almacenamiento del modelo y los codificadores**"
      ],
      "metadata": {
        "id": "upWyR7FOm0zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model and encoders\n",
        "##############################################################################\n",
        "pickle.dump(scaler, open('scaler.sav', 'wb'))\n",
        "pickle.dump(Xenc, open('featuresEncoder.sav', 'wb'))\n",
        "pickle.dump(classifier, open('classifier.pkl', 'wb'))\n"
      ],
      "metadata": {
        "id": "xZL1Zemsm9nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Resultados y metricas**\n",
        "\n",
        "Se utilizó una matriz de confusión para expreser la exactitud del modelo al utilizar los datos de prueba."
      ],
      "metadata": {
        "id": "aUWJLiSonB7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "##############################################################################\n",
        "cm = confusion_matrix(yTest, predictions)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix = cm)\n",
        "display.plot()\n",
        "plt.savefig(\"cm.svg\", format='svg')\n",
        "\n",
        "# K-fold cross validation\n",
        "##############################################################################\n",
        "accuracies = cross_val_score(\n",
        "    estimator = classifier, X = xTrain, y=yTrain, cv=10)\n",
        "\n",
        "print(accuracies.mean())\n"
      ],
      "metadata": {
        "id": "m9z6mdDTnYgq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}